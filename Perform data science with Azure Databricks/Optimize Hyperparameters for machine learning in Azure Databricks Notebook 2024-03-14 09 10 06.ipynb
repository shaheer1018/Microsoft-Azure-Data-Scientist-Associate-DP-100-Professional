{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d787dd-10ba-445f-b037-87d02370e6c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/dbfs/hyperopt_lab': No such file or directory\n--2024-03-14 04:15:12--  https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/penguins.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9533 (9.3K) [text/plain]\nSaving to: ‘/dbfs/hyperopt_lab/penguins.csv’\n\n     0K .........                                             100% 1.34M=0.007s\n\n2024-03-14 04:15:13 (1.34 MB/s) - ‘/dbfs/hyperopt_lab/penguins.csv’ saved [9533/9533]\n\n"
     ]
    }
   ],
   "source": [
    " %sh\n",
    " rm -r /dbfs/hyperopt_lab\n",
    " mkdir /dbfs/hyperopt_lab\n",
    " wget -O /dbfs/hyperopt_lab/penguins.csv https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/penguins.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49522c2f-0bd2-489e-bce7-95a2b25906d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Island</th><th>CulmenLength</th><th>CulmenDepth</th><th>FlipperLength</th><th>BodyMass</th><th>Species</th></tr></thead><tbody><tr><td>Torgersen</td><td>40.29999923706055</td><td>18.0</td><td>195.0</td><td>3250.0</td><td>0</td></tr><tr><td>Torgersen</td><td>38.599998474121094</td><td>21.200000762939453</td><td>191.0</td><td>3800.0</td><td>0</td></tr><tr><td>Biscoe</td><td>37.79999923706055</td><td>18.299999237060547</td><td>174.0</td><td>3400.0</td><td>0</td></tr><tr><td>Biscoe</td><td>35.29999923706055</td><td>18.899999618530273</td><td>187.0</td><td>3800.0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Torgersen",
         40.29999923706055,
         18.0,
         195.0,
         3250.0,
         0
        ],
        [
         "Torgersen",
         38.599998474121094,
         21.200000762939453,
         191.0,
         3800.0,
         0
        ],
        [
         "Biscoe",
         37.79999923706055,
         18.299999237060547,
         174.0,
         3400.0,
         0
        ],
        [
         "Biscoe",
         35.29999923706055,
         18.899999618530273,
         187.0,
         3800.0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Island",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CulmenLength",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "CulmenDepth",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "FlipperLength",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "BodyMass",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Species",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/hyperopt_lab/penguins.csv\")\n",
    "\n",
    "data = data.dropna().select(col(\"Island\").astype(\"string\"),\n",
    "                            col(\"CulmenLength\").astype(\"float\"),\n",
    "                            col(\"CulmenDepth\").astype(\"float\"),\n",
    "                            col(\"FlipperLength\").astype(\"float\"),\n",
    "                            col(\"BodyMass\").astype(\"float\"),\n",
    "                            col(\"Species\").astype(\"int\"))\n",
    "\n",
    "display(data.sample(0.2).head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ac5b5a-8bdd-4e8c-89e0-d116a5fc6bc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Rows:  234 Testing Rows:  108\n"
     ]
    }
   ],
   "source": [
    "# Split the data into two datasets: One for training, and another for testing\n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "print(\"Training Rows: \", train.count(), \"Testing Rows: \", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29615347-a3df-43b1-8a00-2d20acad5292",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Optimize hyperparameter values for training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e990d09b-3367-44ca-b4d4-503d7d0d52a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "import mlflow\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    # Train a model using the provided hyperparameter value\n",
    "    catFeature = \"Island\"\n",
    "    numFeatures = [\"CulmenLength\", \"CulmenDepth\", \"FlipperLength\", \"BodyMass\"]\n",
    "    catIndexer = StringIndexer(inputCol=catFeature, outputCol=catFeature + \"Idx\")\n",
    "    numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "    numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "    featureVector = VectorAssembler(inputCols=[\"IslandIdx\", \"normalizedFeatures\"], outputCol=\"Features\")\n",
    "    mlAlgo = DecisionTreeClassifier(labelCol=\"Species\",    \n",
    "                                    featuresCol=\"Features\",\n",
    "                                    maxDepth=params['MaxDepth'], maxBins=params['MaxBins'])\n",
    "    pipeline = Pipeline(stages=[catIndexer, numVector, numScaler, featureVector, mlAlgo])\n",
    "    model = pipeline.fit(train)\n",
    "       \n",
    "    # Evaluate the model to get the target metric\n",
    "    prediction = model.transform(test)\n",
    "    eval = MulticlassClassificationEvaluator(labelCol=\"Species\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = eval.evaluate(prediction)\n",
    "       \n",
    "    # Hyperopt tries to minimize the objective function, so you must return the negative accuracy.\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e140229-45b0-4be3-bea9-343dde72a684",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/6 [00:00<?, ?trial/s, best loss=?]\r 17%|█▋        | 1/6 [00:06<00:32,  6.48s/trial, best loss: -0.9722222222222222]\r 33%|███▎      | 2/6 [00:09<00:18,  4.55s/trial, best loss: -0.9907407407407407]\r 50%|█████     | 3/6 [00:12<00:11,  3.87s/trial, best loss: -0.9907407407407407]\r 67%|██████▋   | 4/6 [00:15<00:06,  3.35s/trial, best loss: -0.9907407407407407]\r 83%|████████▎ | 5/6 [00:17<00:03,  3.07s/trial, best loss: -0.9907407407407407]\r100%|██████████| 6/6 [00:20<00:00,  2.78s/trial, best loss: -0.9907407407407407]\r100%|██████████| 6/6 [00:20<00:00,  3.35s/trial, best loss: -0.9907407407407407]\nBest param values:  {'MaxBins': 0, 'MaxDepth': 7}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "   \n",
    "# Define a search space for two hyperparameters (maxDepth and maxBins)\n",
    "search_space = {\n",
    "    'MaxDepth': hp.randint('MaxDepth', 10),\n",
    "    'MaxBins': hp.choice('MaxBins', [10, 20, 30])\n",
    "}\n",
    "   \n",
    "# Specify an algorithm for the hyperparameter optimization process\n",
    "algo=tpe.suggest\n",
    "   \n",
    "# Call the training function iteratively to find the optimal hyperparameter values\n",
    "argmin = fmin(\n",
    "  fn=objective,\n",
    "  space=search_space,\n",
    "  algo=algo,\n",
    "  max_evals=6)\n",
    "   \n",
    "print(\"Best param values: \", argmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9a418a4-7b74-4743-ad3c-65addd21765f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Use the Trials class to log run details\n",
    "In addition to using MLflow experiment runs to log details of each iteration, you can also use the hyperopt.Trials class to record and view details of each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c4e6f1-3fac-411c-9375-4a6ba11fa483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/3 [00:00<?, ?trial/s, best loss=?]\r 33%|███▎      | 1/3 [00:02<00:05,  2.68s/trial, best loss: -0.9814814814814815]\r 67%|██████▋   | 2/3 [00:05<00:02,  2.65s/trial, best loss: -0.9814814814814815]\r100%|██████████| 3/3 [00:07<00:00,  2.30s/trial, best loss: -0.9814814814814815]\r100%|██████████| 3/3 [00:07<00:00,  2.40s/trial, best loss: -0.9814814814814815]\nBest param values:  {'MaxBins': 2, 'MaxDepth': 9}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials\n",
    "   \n",
    "# Create a Trials object to track each run\n",
    "trial_runs = Trials()\n",
    "   \n",
    "argmin = fmin(\n",
    "  fn=objective,\n",
    "  space=search_space,\n",
    "  algo=algo,\n",
    "  max_evals=3,\n",
    "  trials=trial_runs)\n",
    "   \n",
    "print(\"Best param values: \", argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40076381-29fd-4b1a-bc0c-69fa0bc7f25a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trials:\n\n {'state': 2, 'tid': 0, 'spec': None, 'result': {'loss': -0.9814814814814815, 'status': 'ok'}, 'misc': {'tid': 0, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'MaxBins': [0], 'MaxDepth': [0]}, 'vals': {'MaxBins': [2], 'MaxDepth': [9]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 3, 14, 4, 57, 55, 658000), 'refresh_time': datetime.datetime(2024, 3, 14, 4, 57, 58, 336000)}\n\n {'state': 2, 'tid': 1, 'spec': None, 'result': {'loss': -0.9629629629629629, 'status': 'ok'}, 'misc': {'tid': 1, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'MaxBins': [1], 'MaxDepth': [1]}, 'vals': {'MaxBins': [1], 'MaxDepth': [9]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 3, 14, 4, 57, 58, 340000), 'refresh_time': datetime.datetime(2024, 3, 14, 4, 58, 0, 966000)}\n\n {'state': 2, 'tid': 2, 'spec': None, 'result': {'loss': -0.9629629629629629, 'status': 'ok'}, 'misc': {'tid': 2, 'cmd': ('domain_attachment', 'FMinIter_Domain'), 'workdir': None, 'idxs': {'MaxBins': [2], 'MaxDepth': [2]}, 'vals': {'MaxBins': [0], 'MaxDepth': [2]}}, 'exp_key': None, 'owner': None, 'version': 0, 'book_time': datetime.datetime(2024, 3, 14, 4, 58, 0, 968000), 'refresh_time': datetime.datetime(2024, 3, 14, 4, 58, 2, 840000)}\n"
     ]
    }
   ],
   "source": [
    "# Get details from each trial run\n",
    "print (\"trials:\")\n",
    "for trial in trial_runs.trials:\n",
    "    print (\"\\n\", trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "180e1f2a-84b3-4781-8435-b4a0f995086e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 27700166799683,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Optimize Hyperparameters for machine learning in Azure Databricks Notebook 2024-03-14 09:10:06",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
