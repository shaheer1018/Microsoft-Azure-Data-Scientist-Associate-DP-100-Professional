{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "318ac1bb-78f7-4a13-87bc-9d621c53a27a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The following code uses shell commands to download the penguin data from GitHub into the Databricks file system (DBFS) used by your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2ad0dd-ee5f-4935-a8ad-63ef36dd7308",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/dbfs/mlflow_lab': No such file or directory\n--2024-03-13 10:44:11--  https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/penguins.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9533 (9.3K) [text/plain]\nSaving to: ‘/dbfs/mlflow_lab/penguins.csv’\n\n     0K .........                                             100% 1.01M=0.009s\n\n2024-03-13 10:44:12 (1.01 MB/s) - ‘/dbfs/mlflow_lab/penguins.csv’ saved [9533/9533]\n\n"
     ]
    }
   ],
   "source": [
    " %sh\n",
    " rm -r /dbfs/mlflow_lab\n",
    " mkdir /dbfs/mlflow_lab\n",
    " wget -O /dbfs/mlflow_lab/penguins.csv https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/penguins.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1058bd-3e33-4fec-ac3e-a8e90d082bff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/mlflow_lab/penguins.csv\")\n",
    "data = data.dropna().select(col(\"Island\").astype(\"string\"),\n",
    "                            col(\"CulmenLength\").astype(\"float\"),\n",
    "                            col(\"CulmenDepth\").astype(\"float\"),\n",
    "                            col(\"FlipperLength\").astype(\"float\"),\n",
    "                            col(\"BodyMass\").astype(\"float\"),\n",
    "                            col(\"Species\").astype(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8722f143-9b4d-49ec-ba0a-ecdedd6e9aaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Island</th><th>CulmenLength</th><th>CulmenDepth</th><th>FlipperLength</th><th>BodyMass</th><th>Species</th></tr></thead><tbody><tr><td>Torgersen</td><td>38.900001525878906</td><td>17.799999237060547</td><td>181.0</td><td>3625.0</td><td>0</td></tr><tr><td>Torgersen</td><td>42.0</td><td>20.200000762939453</td><td>190.0</td><td>4250.0</td><td>0</td></tr><tr><td>Torgersen</td><td>41.099998474121094</td><td>17.600000381469727</td><td>182.0</td><td>3200.0</td><td>0</td></tr><tr><td>Torgersen</td><td>36.599998474121094</td><td>17.799999237060547</td><td>185.0</td><td>3700.0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Torgersen",
         38.900001525878906,
         17.799999237060547,
         181.0,
         3625.0,
         0
        ],
        [
         "Torgersen",
         42.0,
         20.200000762939453,
         190.0,
         4250.0,
         0
        ],
        [
         "Torgersen",
         41.099998474121094,
         17.600000381469727,
         182.0,
         3200.0,
         0
        ],
        [
         "Torgersen",
         36.599998474121094,
         17.799999237060547,
         185.0,
         3700.0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Island",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CulmenLength",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "CulmenDepth",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "FlipperLength",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "BodyMass",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Species",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.sample(0.2).head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29cae5c-eb4b-4d76-b1e9-a473b696b111",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Rows: 243  Testing Rows: 99\n"
     ]
    }
   ],
   "source": [
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c76ed445-2114-407b-ae17-e4b134b4d4aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Run an MLflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9b46f8-43b7-4931-8115-aa88a91b88a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model...\naccuracy: 0.9090909090909091\nweightedRecall: 0.9090909090909091\nweightedPrecision: 0.9245283018867925\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment run complete.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import time\n",
    "\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    catFeature = \"Island\"\n",
    "    numFeatures = [\"CulmenLength\", \"CulmenDepth\", \"FlipperLength\", \"BodyMass\"]\n",
    "     \n",
    "    # parameters\n",
    "    maxIterations = 5\n",
    "    regularization = 0.5\n",
    "   \n",
    "    # Define the feature engineering and model steps\n",
    "    catIndexer = StringIndexer(inputCol=catFeature, outputCol=catFeature + \"Idx\")\n",
    "    numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "    numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "    featureVector = VectorAssembler(inputCols=[\"IslandIdx\", \"normalizedFeatures\"], outputCol=\"Features\")\n",
    "    algo = LogisticRegression(labelCol=\"Species\", featuresCol=\"Features\", maxIter=maxIterations, regParam=regularization)\n",
    "   \n",
    "    # Chain the steps as stages in a pipeline\n",
    "    pipeline = Pipeline(stages=[catIndexer, numVector, numScaler, featureVector, algo])\n",
    "   \n",
    "    # Log training parameter values\n",
    "    print (\"Training Logistic Regression model...\")\n",
    "    mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "    mlflow.log_param('regParam', algo.getRegParam())\n",
    "    model = pipeline.fit(train)\n",
    "      \n",
    "    # Evaluate the model and log metrics\n",
    "    prediction = model.transform(test)\n",
    "    metrics = [\"accuracy\", \"weightedRecall\", \"weightedPrecision\"]\n",
    "    for metric in metrics:\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"Species\", predictionCol=\"prediction\", metricName=metric)\n",
    "        metricValue = evaluator.evaluate(prediction)\n",
    "        print(\"%s: %s\" % (metric, metricValue))\n",
    "        mlflow.log_metric(metric, metricValue)\n",
    "\n",
    "\n",
    "    # Log the model itself\n",
    "    unique_model_name = \"classifier-\" + str(time.time())\n",
    "    mlflow.spark.log_model(model, unique_model_name, mlflow.spark.get_default_conda_env())\n",
    "    modelpath = \"/model/%s\" % (unique_model_name)\n",
    "    mlflow.spark.save_model(model, modelpath)\n",
    "       \n",
    "    print(\"Experiment run complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1188ad68-8e02-44a8-8267-264241ac6229",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b5822b8-ce68-44d6-aded-308028e2e8cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In machine learning projects, data scientists often try training models with different parameters, logging the results each time. To accomplish that, it’s common to create a function that encapsulates the training process and call it with the parameters you want to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f6cd406-c79d-44a3-b488-306e17a47f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_penguin_model(training_data, test_data, maxIterations, regularization):\n",
    "    import mlflow\n",
    "    import mlflow.spark\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    import time\n",
    "   \n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run():\n",
    "   \n",
    "        catFeature = \"Island\"\n",
    "        numFeatures = [\"CulmenLength\", \"CulmenDepth\", \"FlipperLength\", \"BodyMass\"]\n",
    "   \n",
    "        # Define the feature engineering and model steps\n",
    "        catIndexer = StringIndexer(inputCol=catFeature, outputCol=catFeature + \"Idx\")\n",
    "        numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "        numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "        featureVector = VectorAssembler(inputCols=[\"IslandIdx\", \"normalizedFeatures\"], outputCol=\"Features\")\n",
    "        algo = LogisticRegression(labelCol=\"Species\", featuresCol=\"Features\", maxIter=maxIterations, regParam=regularization)\n",
    "   \n",
    "        # Chain the steps as stages in a pipeline\n",
    "        pipeline = Pipeline(stages=[catIndexer, numVector, numScaler, featureVector, algo])\n",
    "   \n",
    "        # Log training parameter values\n",
    "        print (\"Training Logistic Regression model...\")\n",
    "        mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "        mlflow.log_param('regParam', algo.getRegParam())\n",
    "        model = pipeline.fit(training_data)\n",
    "   \n",
    "        # Evaluate the model and log metrics\n",
    "        prediction = model.transform(test_data)\n",
    "        metrics = [\"accuracy\", \"weightedRecall\", \"weightedPrecision\"]\n",
    "        for metric in metrics:\n",
    "            evaluator = MulticlassClassificationEvaluator(labelCol=\"Species\", predictionCol=\"prediction\", metricName=metric)\n",
    "            metricValue = evaluator.evaluate(prediction)\n",
    "            print(\"%s: %s\" % (metric, metricValue))\n",
    "            mlflow.log_metric(metric, metricValue)\n",
    "   \n",
    "   \n",
    "        # Log the model itself\n",
    "        unique_model_name = \"classifier-\" + str(time.time())\n",
    "        mlflow.spark.log_model(model, unique_model_name, mlflow.spark.get_default_conda_env())\n",
    "        modelpath = \"/model/%s\" % (unique_model_name)\n",
    "        mlflow.spark.save_model(model, modelpath)\n",
    "   \n",
    "        print(\"Experiment run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b094ba8-08d2-4811-94fe-d82f6bb145f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model...\naccuracy: 0.9292929292929293\nweightedRecall: 0.9292929292929293\nweightedPrecision: 0.9389978213507626\nExperiment run complete.\n"
     ]
    }
   ],
   "source": [
    "train_penguin_model(train, test, 10, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b24c985f-0658-40a2-9c5e-0c9f0fc5b84c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Register and deploy a model with MLflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51a55e4d-c4f2-496d-90e3-7f0a66821f37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. View the details page for the most recent experiment run.\n",
    "2. Use the Register Model button to register the model that was logged in that experiment and when prompted, create a new model named Penguin Predictor.\n",
    "3. When the model has been registered, view the Models page (in the navigation bar on the left) and select the Penguin Predictor model.\n",
    "4. In the page for the Penguin Predictor model, use the Use model for inference button to create a new real-time endpoint with the following settings: \n",
    "\n",
    "    Model: Penguin Predictor,\n",
    "    Model version: 1,\n",
    "    Endpoint: predict-penguin,\n",
    "    Compute size: Small\n",
    "The serving endpoint is hosted in a new cluster, which it may take several minutes to create.\n",
    "\n",
    "5. When the endpoint has been created, use the Query endpoint button at the top right to open an interface from which you can test the endpoint. Then in the test interface, on the Browser tab, enter the following JSON request and use the Send Request button to call the endpoint and generate a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b8b02a3-ce68-4bd5-8e0e-cef0163276ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "   \"dataframe_records\": [\n",
    "   {\n",
    "      \"Island\": \"Biscoe\",\n",
    "      \"CulmenLength\": 48.7,\n",
    "      \"CulmenDepth\": 14.1,\n",
    "      \"FlipperLength\": 210,\n",
    "      \"BodyMass\": 4450\n",
    "   }\n",
    "   ]\n",
    " }"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1975263188207339,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Use MLflow in Azure Databricks Notebook 2024-03-13 15:34:22",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
